{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharlotteManganye/Deep-Learning-CNN/blob/main/Cfar100_Wider_net_HLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74fd2460",
      "metadata": {
        "id": "74fd2460"
      },
      "source": [
        "https://github.com/WinterWen666/Efficient-High-Order-Volterra-Convolution/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import CIFAR100\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Function\n",
        "# import import_ipynb\n",
        "# from Efficient_High_order_conv import PM_creation, PCM_creation, HO_conv\n",
        "from torchvision.transforms.autoaugment import AutoAugmentPolicy\n"
      ],
      "metadata": {
        "id": "uZkEjWpgt_Ib"
      },
      "id": "uZkEjWpgt_Ib",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficient_High_order_conv"
      ],
      "metadata": {
        "id": "uXez9K8ju91C"
      },
      "id": "uXez9K8ju91C"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def PM_creation( N, input_size, TRM):\n",
        "    index = []\n",
        "    PM = torch.arange(input_size)[:,None]\n",
        "    TPM = []\n",
        "    TPM.append(PM)\n",
        "    PM_full = []\n",
        "    for i in range(N-1):\n",
        "        Nc = int(PM.shape[1])\n",
        "        for j in range(input_size-Nc):\n",
        "            j = j+1\n",
        "            p_N1 = (PM[:,Nc-1] + j)[:, None]\n",
        "            index_chosen = torch.argwhere(p_N1 < input_size)\n",
        "            index_final = torch.concat((PM[index_chosen[:,0]],p_N1[index_chosen[:,0]]), axis = 1)\n",
        "            index.append(index_final)\n",
        "            p_N1 = p_N1 - j\n",
        "        PM = torch.vstack(index)\n",
        "        index = []\n",
        "        TPM.append(PM)\n",
        "    for k in range(len(TRM)):\n",
        "        PM = TPM[len(TRM[k])-1]\n",
        "        PM_full.append(np.repeat(PM, TRM[k], axis =1))\n",
        "    PM_full = torch.vstack(PM_full)\n",
        "    return PM_full\n",
        "\n",
        "\n",
        "def PCM_creation(PM_full_r_1, PM_full_r):\n",
        "    PCM_r = []\n",
        "    PCMs_r = []\n",
        "    for i in range(PM_full_r.shape[1]):\n",
        "        PM_deleted = torch.cat((PM_full_r[:, :i], PM_full_r[:, i + 1:]), dim=1)\n",
        "        for row_r in PM_deleted:\n",
        "            Position = torch.where(torch.all(PM_full_r_1 ==row_r, axis = 1))\n",
        "            PCM_r.append(torch.tensor(Position))\n",
        "        PCM_r = torch.vstack(PCM_r)\n",
        "        PCM_r = torch.concat((PM_full_r[:,i][:,None], PCM_r), axis = 1)\n",
        "        PCMs_r.append(PCM_r)\n",
        "        PCM_r = []\n",
        "    return  PCMs_r\n",
        "\n",
        "\n",
        "class HT_creation(Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input, kernel_size, stride, padding,\n",
        "                  TPCMs_r):\n",
        "            input = input.to(device)\n",
        "            shape = input.shape\n",
        "            grad_move = []\n",
        "            grad_move.append(0)\n",
        "            x_r_1_move =[]\n",
        "            x_r_1_move.append(0)\n",
        "            HT = []\n",
        "            unfold = nn.Unfold(kernel_size = kernel_size,  padding = padding, stride = stride)\n",
        "            Col = unfold(input)\n",
        "            weight_shape = []\n",
        "\n",
        "            ori_col = int(kernel_size[0]*kernel_size[1])\n",
        "            Col = Col.view(shape[0], shape[1], ori_col, Col.shape[-1])\n",
        "\n",
        "            for i in range(len(TPCMs_r)):\n",
        "                if i == 0:\n",
        "                    grad_move.append(grad_move[i]+TPCMs_r[i][0].shape[0])\n",
        "                    weight_shape.append((ori_col, ori_col))\n",
        "                    HT.append(Col[:,:,TPCMs_r[i][0][:,0],:]*Col[:,:,TPCMs_r[i][0][:,1],:])\n",
        "                    x_r_1_move.append(x_r_1_move[i]+Col.shape[2])\n",
        "                else:\n",
        "                    grad_move.append(grad_move[i]+TPCMs_r[i][0].shape[0])\n",
        "                    weight_shape.append((TPCMs_r[i-1][0].shape[0], ori_col))\n",
        "                    HT.append(Col[:,:,TPCMs_r[i][0][:,0],:]*HT[i-1][:,:,TPCMs_r[i][0][:,1],:])\n",
        "                    x_r_1_move.append(x_r_1_move[i]+TPCMs_r[i-1][0].shape[0])\n",
        "            HT = torch.cat(HT, dim = 2)\n",
        "            if len(TPCMs_r) == 1:\n",
        "                # ctx.back_terms = Col\n",
        "                ctx.save_for_backward(Col)\n",
        "            else:\n",
        "                # ctx.back_terms = torch.cat((Col,HT[:,:,0:grad_move[-2],:]), dim=2 )\n",
        "                ctx.save_for_backward(torch.cat((Col,HT[:,:,0:grad_move[-2],:]), dim=2 ))\n",
        "            ctx.grad_move = grad_move\n",
        "            ctx.x_r_1_move = x_r_1_move\n",
        "            ctx.TPCMs_r = TPCMs_r\n",
        "\n",
        "            ctx.weight_shape = weight_shape\n",
        "            ctx.kernel_size =kernel_size\n",
        "            ctx.stride = stride\n",
        "            ctx.padding = padding\n",
        "            ctx.shape =shape\n",
        "\n",
        "            return HT\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad):\n",
        "            (back_terms,) = ctx.saved_tensors\n",
        "\n",
        "            # back_terms = ctx.back_terms\n",
        "            weight_shape = ctx.weight_shape\n",
        "            TPCMs_r = ctx.TPCMs_r\n",
        "            kernel_size = ctx.kernel_size\n",
        "            stride = ctx.stride\n",
        "            padding = ctx.padding\n",
        "            shape = ctx.shape\n",
        "            grad_move = ctx.grad_move\n",
        "            x_r_1_move = ctx.x_r_1_move\n",
        "            grad_input  = None\n",
        "\n",
        "            for i in range(len(TPCMs_r)):\n",
        "                Al_grad = torch.zeros((grad.shape[0], grad.shape[1],\n",
        "                                          weight_shape[i][0], weight_shape[i][1],grad.shape[-1])).to(device)\n",
        "                if i == 0:\n",
        "                    Al_grad[:,:, TPCMs_r[0][0][:,0],TPCMs_r[0][0][:,1],:] = grad[:,:,grad_move[0]:grad_move[1],:]\n",
        "                    # Al_grad[:,:, PCMs_r[0][:,1],PCMs_r[0][:,0],:] += grad[:,:,grad_move[0]:grad_move[1],:]\n",
        "                    Al_grad = Al_grad + Al_grad.transpose(2,3)\n",
        "                    grad_input = torch.einsum('bcikj,bckj->bcij',Al_grad,\n",
        "                                              back_terms[:,:,x_r_1_move[0]:x_r_1_move[1],:])\n",
        "                else:\n",
        "                    Al_grad[:,:, TPCMs_r[i][0][:,1], TPCMs_r[i][0][:,0],:] = grad[:,:,grad_move[i]:grad_move[i+1],:]\n",
        "                    for j in range(i+1):\n",
        "                        Al_grad[:,:, TPCMs_r[i][j+1][:,1], TPCMs_r[i][j+1][:,0],:] +=grad[:,:,grad_move[i]:grad_move[i+1],:]\n",
        "                    grad_input += torch.einsum('bckij,bckj->bcij',Al_grad,\n",
        "                                                back_terms[:,:,x_r_1_move[i]:x_r_1_move[i+1],:])\n",
        "            grad_input = grad_input.reshape(grad_input.shape[0],\n",
        "                                          grad_input.shape[1]*grad_input.shape[2], grad_input.shape[-1])\n",
        "            fold = nn.Fold((shape[-2], shape[-1]), kernel_size =kernel_size, stride = stride, padding = padding )\n",
        "            grad_input = fold(grad_input)\n",
        "\n",
        "            return grad_input, None, None, None, None, None\n",
        "\n",
        "class high_order_input(nn.Module):\n",
        "    def __init__(self, kernel_size, stride, padding, TPCMs_r):\n",
        "        super().__init__()\n",
        "        self.func = HT_creation\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.TPCMs_r = TPCMs_r\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.func.apply(input,\n",
        "                          self.kernel_size,\n",
        "                          self.stride,\n",
        "                          self.padding,\n",
        "                          self.TPCMs_r\n",
        "                          )\n",
        "\n",
        "\n",
        "class HO_conv(nn.Module):\n",
        "    def __init__(self, in_chan , out_chan, kernel_size, stride, padding, groups, TPCMs_r):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding =  padding\n",
        "        self.stride = stride\n",
        "        self.TPCMs_r = TPCMs_r\n",
        "        self.H_kernel_size = 0\n",
        "        for i in range(len(self.TPCMs_r)):\n",
        "\n",
        "            size = self.TPCMs_r[i][0].shape[0]\n",
        "\n",
        "            self.H_kernel_size += size\n",
        "        self.conv =  nn.Sequential( high_order_input( self.kernel_size,\n",
        "                                                self.stride,\n",
        "                                                self.padding,\n",
        "                                                self.TPCMs_r),\n",
        "                                    nn.Conv2d(in_chan, out_chan,\n",
        "                                              kernel_size=(self.H_kernel_size,1),\n",
        "                                              stride = 1,\n",
        "                                              groups = groups, bias = False))\n",
        "    def forward(self,input):\n",
        "        in_shape = input.shape\n",
        "        x = self.conv(input)\n",
        "        x =  x.reshape(x.shape[0],\n",
        "                        x.shape[1],\n",
        "                      (in_shape[-2]-self.kernel_size[0]+ 2* self.padding[0])//self.stride[0] + 1,\n",
        "                      (in_shape[-1]-self.kernel_size[1]+ 2* self.padding[1])//self.stride[1] + 1 )\n",
        "        return x"
      ],
      "metadata": {
        "id": "OiVfLfFsuKHS"
      },
      "id": "OiVfLfFsuKHS",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N3V8nqpkvF2p"
      },
      "id": "N3V8nqpkvF2p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311c21c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "311c21c8",
        "outputId": "2306fdc0-ca6f-4cc9-fae0-d492a02d7540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:02<00:00, 63107423.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch [1/200], Step [391/391], Loss: 3.2563, train_acc: 11.430 %\n",
            "Accuracy of the network on the 10000 test images: 15.7 %\n",
            "Epoch [2/200], Step [391/391], Loss: 2.6777, train_acc: 25.082 %\n",
            "Accuracy of the network on the 10000 test images: 27.79 %\n",
            "Epoch [3/200], Step [391/391], Loss: 2.1976, train_acc: 36.474 %\n",
            "Accuracy of the network on the 10000 test images: 36.49 %\n",
            "Epoch [4/200], Step [391/391], Loss: 1.8147, train_acc: 44.496 %\n",
            "Accuracy of the network on the 10000 test images: 41.51 %\n",
            "Epoch [5/200], Step [391/391], Loss: 1.6855, train_acc: 50.368 %\n",
            "Accuracy of the network on the 10000 test images: 40.38 %\n",
            "Epoch [6/200], Step [391/391], Loss: 1.5218, train_acc: 54.448 %\n",
            "Accuracy of the network on the 10000 test images: 46.47 %\n",
            "Epoch [7/200], Step [391/391], Loss: 1.3196, train_acc: 57.970 %\n",
            "Accuracy of the network on the 10000 test images: 53.07 %\n",
            "Epoch [8/200], Step [391/391], Loss: 1.6762, train_acc: 60.606 %\n",
            "Accuracy of the network on the 10000 test images: 51.18 %\n",
            "Epoch [9/200], Step [391/391], Loss: 1.3000, train_acc: 62.714 %\n",
            "Accuracy of the network on the 10000 test images: 52.2 %\n",
            "Epoch [10/200], Step [391/391], Loss: 1.6727, train_acc: 64.172 %\n",
            "Accuracy of the network on the 10000 test images: 54.13 %\n",
            "Epoch [11/200], Step [391/391], Loss: 1.3399, train_acc: 65.464 %\n",
            "Accuracy of the network on the 10000 test images: 56.59 %\n",
            "Epoch [12/200], Step [391/391], Loss: 1.2817, train_acc: 66.688 %\n",
            "Accuracy of the network on the 10000 test images: 54.18 %\n",
            "Epoch [13/200], Step [391/391], Loss: 1.1698, train_acc: 67.708 %\n",
            "Accuracy of the network on the 10000 test images: 55.96 %\n",
            "Epoch [14/200], Step [391/391], Loss: 1.2644, train_acc: 68.786 %\n",
            "Accuracy of the network on the 10000 test images: 60.12 %\n",
            "Epoch [15/200], Step [391/391], Loss: 1.2232, train_acc: 69.352 %\n",
            "Accuracy of the network on the 10000 test images: 53.76 %\n",
            "Epoch [16/200], Step [391/391], Loss: 0.8971, train_acc: 70.138 %\n",
            "Accuracy of the network on the 10000 test images: 58.54 %\n",
            "Epoch [17/200], Step [391/391], Loss: 0.9456, train_acc: 70.556 %\n",
            "Accuracy of the network on the 10000 test images: 56.18 %\n",
            "Epoch [18/200], Step [391/391], Loss: 1.2392, train_acc: 71.406 %\n",
            "Accuracy of the network on the 10000 test images: 55.34 %\n",
            "Epoch [19/200], Step [391/391], Loss: 0.7370, train_acc: 71.820 %\n",
            "Accuracy of the network on the 10000 test images: 60.38 %\n",
            "Epoch [20/200], Step [391/391], Loss: 0.7875, train_acc: 72.554 %\n",
            "Accuracy of the network on the 10000 test images: 58.2 %\n",
            "Epoch [21/200], Step [391/391], Loss: 1.0267, train_acc: 72.572 %\n",
            "Accuracy of the network on the 10000 test images: 55.21 %\n",
            "Epoch [22/200], Step [391/391], Loss: 0.9846, train_acc: 72.932 %\n",
            "Accuracy of the network on the 10000 test images: 58.32 %\n",
            "Epoch [23/200], Step [391/391], Loss: 1.0111, train_acc: 73.700 %\n",
            "Accuracy of the network on the 10000 test images: 53.94 %\n",
            "Epoch [24/200], Step [391/391], Loss: 0.9259, train_acc: 73.682 %\n",
            "Accuracy of the network on the 10000 test images: 57.55 %\n",
            "Epoch [25/200], Step [391/391], Loss: 0.8835, train_acc: 74.078 %\n",
            "Accuracy of the network on the 10000 test images: 55.68 %\n",
            "Epoch [26/200], Step [391/391], Loss: 0.7932, train_acc: 73.994 %\n",
            "Accuracy of the network on the 10000 test images: 58.91 %\n",
            "Epoch [27/200], Step [391/391], Loss: 0.9775, train_acc: 74.188 %\n",
            "Accuracy of the network on the 10000 test images: 56.85 %\n",
            "Epoch [28/200], Step [391/391], Loss: 0.7600, train_acc: 74.682 %\n",
            "Accuracy of the network on the 10000 test images: 57.01 %\n",
            "Epoch [29/200], Step [391/391], Loss: 1.0281, train_acc: 74.634 %\n",
            "Accuracy of the network on the 10000 test images: 58.88 %\n",
            "Epoch [30/200], Step [391/391], Loss: 0.9108, train_acc: 75.116 %\n",
            "Accuracy of the network on the 10000 test images: 55.61 %\n",
            "Epoch [31/200], Step [391/391], Loss: 0.6281, train_acc: 75.086 %\n",
            "Accuracy of the network on the 10000 test images: 61.24 %\n",
            "Epoch [32/200], Step [391/391], Loss: 0.6495, train_acc: 75.368 %\n",
            "Accuracy of the network on the 10000 test images: 56.06 %\n",
            "Epoch [33/200], Step [391/391], Loss: 0.8988, train_acc: 75.426 %\n",
            "Accuracy of the network on the 10000 test images: 61.7 %\n",
            "Epoch [34/200], Step [391/391], Loss: 0.8837, train_acc: 76.014 %\n",
            "Accuracy of the network on the 10000 test images: 60.11 %\n",
            "Epoch [35/200], Step [391/391], Loss: 1.0433, train_acc: 75.996 %\n",
            "Accuracy of the network on the 10000 test images: 62.55 %\n",
            "Epoch [36/200], Step [391/391], Loss: 0.5822, train_acc: 75.988 %\n",
            "Accuracy of the network on the 10000 test images: 56.95 %\n",
            "Epoch [37/200], Step [391/391], Loss: 1.0522, train_acc: 75.908 %\n",
            "Accuracy of the network on the 10000 test images: 59.34 %\n",
            "Epoch [38/200], Step [391/391], Loss: 0.9166, train_acc: 76.326 %\n",
            "Accuracy of the network on the 10000 test images: 53.39 %\n",
            "Epoch [39/200], Step [391/391], Loss: 0.7707, train_acc: 76.260 %\n",
            "Accuracy of the network on the 10000 test images: 58.93 %\n",
            "Epoch [40/200], Step [391/391], Loss: 0.7109, train_acc: 76.644 %\n",
            "Accuracy of the network on the 10000 test images: 61.22 %\n",
            "Epoch [41/200], Step [391/391], Loss: 0.9464, train_acc: 76.462 %\n",
            "Accuracy of the network on the 10000 test images: 59.57 %\n",
            "Epoch [42/200], Step [391/391], Loss: 0.9727, train_acc: 76.606 %\n",
            "Accuracy of the network on the 10000 test images: 58.61 %\n",
            "Epoch [43/200], Step [391/391], Loss: 0.8167, train_acc: 76.648 %\n",
            "Accuracy of the network on the 10000 test images: 57.13 %\n",
            "Epoch [44/200], Step [391/391], Loss: 0.8246, train_acc: 76.910 %\n",
            "Accuracy of the network on the 10000 test images: 58.83 %\n",
            "Epoch [45/200], Step [391/391], Loss: 0.7580, train_acc: 76.718 %\n",
            "Accuracy of the network on the 10000 test images: 62.48 %\n",
            "Epoch [46/200], Step [391/391], Loss: 0.8309, train_acc: 77.120 %\n",
            "Accuracy of the network on the 10000 test images: 60.76 %\n",
            "Epoch [47/200], Step [391/391], Loss: 0.7707, train_acc: 77.030 %\n",
            "Accuracy of the network on the 10000 test images: 61.23 %\n",
            "Epoch [48/200], Step [391/391], Loss: 0.9580, train_acc: 77.426 %\n",
            "Accuracy of the network on the 10000 test images: 57.69 %\n",
            "Epoch [49/200], Step [391/391], Loss: 0.8275, train_acc: 76.988 %\n",
            "Accuracy of the network on the 10000 test images: 60.12 %\n",
            "Epoch [50/200], Step [391/391], Loss: 0.6457, train_acc: 77.128 %\n",
            "Accuracy of the network on the 10000 test images: 61.91 %\n",
            "Epoch [51/200], Step [391/391], Loss: 0.9146, train_acc: 77.424 %\n",
            "Accuracy of the network on the 10000 test images: 61.28 %\n",
            "Epoch [52/200], Step [391/391], Loss: 0.7565, train_acc: 77.392 %\n",
            "Accuracy of the network on the 10000 test images: 62.86 %\n",
            "Epoch [53/200], Step [391/391], Loss: 0.9542, train_acc: 77.294 %\n",
            "Accuracy of the network on the 10000 test images: 58.49 %\n",
            "Epoch [54/200], Step [391/391], Loss: 0.9754, train_acc: 77.614 %\n",
            "Accuracy of the network on the 10000 test images: 60.31 %\n",
            "Epoch [55/200], Step [391/391], Loss: 0.7845, train_acc: 77.736 %\n",
            "Accuracy of the network on the 10000 test images: 59.09 %\n",
            "Epoch [56/200], Step [391/391], Loss: 0.9256, train_acc: 77.902 %\n",
            "Accuracy of the network on the 10000 test images: 58.71 %\n",
            "Epoch [57/200], Step [391/391], Loss: 0.7325, train_acc: 77.356 %\n",
            "Accuracy of the network on the 10000 test images: 59.35 %\n",
            "Epoch [58/200], Step [391/391], Loss: 0.7563, train_acc: 77.548 %\n",
            "Accuracy of the network on the 10000 test images: 60.63 %\n",
            "Epoch [59/200], Step [391/391], Loss: 0.7715, train_acc: 77.948 %\n",
            "Accuracy of the network on the 10000 test images: 60.05 %\n",
            "Epoch [60/200], Step [391/391], Loss: 0.8132, train_acc: 77.678 %\n",
            "Accuracy of the network on the 10000 test images: 55.39 %\n",
            "Epoch [61/200], Step [391/391], Loss: 0.1757, train_acc: 90.196 %\n",
            "Accuracy of the network on the 10000 test images: 77.99 %\n",
            "Epoch [62/200], Step [391/391], Loss: 0.1759, train_acc: 94.342 %\n",
            "Accuracy of the network on the 10000 test images: 78.09 %\n",
            "Epoch [63/200], Step [391/391], Loss: 0.3339, train_acc: 95.796 %\n",
            "Accuracy of the network on the 10000 test images: 78.42 %\n",
            "Epoch [64/200], Step [391/391], Loss: 0.1307, train_acc: 96.794 %\n",
            "Accuracy of the network on the 10000 test images: 78.29 %\n",
            "Epoch [65/200], Step [391/391], Loss: 0.1449, train_acc: 97.568 %\n",
            "Accuracy of the network on the 10000 test images: 79.16 %\n",
            "Epoch [66/200], Step [391/391], Loss: 0.1178, train_acc: 97.896 %\n",
            "Accuracy of the network on the 10000 test images: 78.99 %\n",
            "Epoch [67/200], Step [391/391], Loss: 0.1714, train_acc: 98.346 %\n",
            "Accuracy of the network on the 10000 test images: 78.6 %\n",
            "Epoch [68/200], Step [391/391], Loss: 0.0864, train_acc: 98.622 %\n",
            "Accuracy of the network on the 10000 test images: 78.35 %\n",
            "Epoch [69/200], Step [391/391], Loss: 0.1079, train_acc: 98.816 %\n",
            "Accuracy of the network on the 10000 test images: 78.56 %\n",
            "Epoch [70/200], Step [391/391], Loss: 0.0773, train_acc: 98.860 %\n",
            "Accuracy of the network on the 10000 test images: 78.59 %\n",
            "Epoch [71/200], Step [391/391], Loss: 0.1214, train_acc: 98.876 %\n",
            "Accuracy of the network on the 10000 test images: 78.62 %\n",
            "Epoch [72/200], Step [391/391], Loss: 0.1213, train_acc: 98.822 %\n",
            "Accuracy of the network on the 10000 test images: 77.66 %\n",
            "Epoch [73/200], Step [391/391], Loss: 0.0955, train_acc: 98.978 %\n",
            "Accuracy of the network on the 10000 test images: 77.22 %\n",
            "Epoch [74/200], Step [391/391], Loss: 0.1024, train_acc: 98.784 %\n",
            "Accuracy of the network on the 10000 test images: 76.8 %\n",
            "Epoch [75/200], Step [391/391], Loss: 0.1232, train_acc: 98.664 %\n",
            "Accuracy of the network on the 10000 test images: 76.61 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "\n",
        "    transforms.RandomCrop(32, padding = 4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.AutoAugment(AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n",
        "    # Cutout(1, 16),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset with data augmentation\n",
        "train_dataset = CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = 100\n",
        "k = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class squeeze(nn.Module):\n",
        "    def __init__(self, in_chan, reduce):\n",
        "        super(squeeze, self).__init__()\n",
        "\n",
        "\n",
        "        self.squ = nn.Sequential(\n",
        "                                    nn.AdaptiveAvgPool2d(1),\n",
        "                                    nn.Conv2d(in_chan, in_chan//reduce,1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(in_chan//reduce,in_chan, 1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "    def forward(self, input):\n",
        "        x = self.squ(input)*input\n",
        "        return x\n",
        "\n",
        "class Volconv(nn.Module):\n",
        "    def __init__(self, in_chan,out_chan, kernel_size, stride, padding, groups):\n",
        "        super(Volconv, self).__init__()\n",
        "        input_size = int(kernel_size[0]*kernel_size[1])\n",
        "        # PM1_Full = torch.arange(input_size)[:,None]\n",
        "        TRM2 = [[2],[1,1]]\n",
        "        PM2_Full = PM_creation(2, input_size, TRM2)\n",
        "        # TRM3 = [[3],[1,2],[2, 1], [ 1, 1, 1]]  # Third order Toe\n",
        "        # PM3_Full = PM_creation(3, input_size, TPE3)\n",
        "        TPCMs_r = [[PM2_Full]]\n",
        "        # PCMs_r = PCM_creation(PM2_Full, PM3_Full)\n",
        "\n",
        "        self.Second_conv = nn.Sequential(\n",
        "\n",
        "                                   HO_conv(in_chan,  out_chan,\n",
        "                                              kernel_size  ,\n",
        "                                              stride  , padding, groups, TPCMs_r\n",
        "                                              ), )\n",
        "\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "                                   nn.Conv2d(in_chan, out_chan, kernel_size,\n",
        "                                              stride , padding, groups = groups, bias = False))\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.Second_conv(input)+self.conv(input)\n",
        "\n",
        "        return x\n",
        "class HLA(nn.Module):\n",
        "    def __init__(self, in_chan, kernel_size, pooling_size, padding,  in_size, reduce):\n",
        "        super(HLA, self).__init__()\n",
        "        up_size1 = in_size[0]//pooling_size[0]\n",
        "        up_size2 = in_size[1]//pooling_size[1]\n",
        "\n",
        "        self.Vol1= nn.Sequential(   nn.BatchNorm2d( in_chan),\n",
        "                                    nn.ReLU(),\n",
        "\n",
        "                                    nn.AdaptiveAvgPool2d((pooling_size[0], pooling_size[1])),\n",
        "\n",
        "                                    nn.Conv2d(in_chan, in_chan//reduce, kernel_size=1,\n",
        "                                                stride=1 , padding = 0, groups = 1),\n",
        "                                    nn.BatchNorm2d( in_chan//reduce),\n",
        "\n",
        "                                    Volconv(in_chan//reduce,in_chan//reduce, kernel_size, (1, 1), padding, 1),\n",
        "\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d( in_chan//reduce),\n",
        "\n",
        "                                    nn.Conv2d(in_chan//reduce, in_chan, kernel_size=1,\n",
        "                                                stride=1 , padding = 0, groups = 1),\n",
        "\n",
        "                                    nn.Sigmoid(),\n",
        "\n",
        "                                    )\n",
        "        self.sq = nn.Sequential(\n",
        "                                    nn.AdaptiveAvgPool2d(1),\n",
        "                                    nn.Conv2d(in_chan, in_chan//reduce, 1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(in_chan//reduce, in_chan, 1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor= (up_size1,up_size2) )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x1 = self.Vol1(input)\n",
        "        x2 = self.sq(input)\n",
        "        x3 = torch.mean(x2, dim=1, keepdim= True)\n",
        "        x2 = x3 - nn.ReLU()(x3 - x2)\n",
        "        x = x2 + x1 - x2*x1\n",
        "        x = self.up(x)*input\n",
        "        return x\n",
        "\n",
        "class Branch(nn.Module):\n",
        "    def __init__(self,\n",
        "                  in_cha,\n",
        "                  out_cha,\n",
        "                  kernel_size,\n",
        "                  stride,\n",
        "                  padding,\n",
        "                  in_size\n",
        "                  ):\n",
        "\n",
        "        super(Branch, self).__init__()\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_cha != out_cha or stride != 1:\n",
        "            self.shortcut = nn.Sequential( nn.Conv2d(in_cha,\n",
        "                        out_cha,\n",
        "                        kernel_size=1,\n",
        "                        stride= stride  , padding=0,\n",
        "                      bias = False),)\n",
        "\n",
        "        self.Branch1_1 = nn.Sequential(\n",
        "                                  nn.BatchNorm2d( in_cha),\n",
        "                                  nn.ReLU(),\n",
        "\n",
        "                                  nn.Conv2d(in_cha,\n",
        "                                            out_cha,\n",
        "                                            kernel_size = 3,\n",
        "                                            stride= 1, padding = 1,\n",
        "                                            bias = False),\n",
        "\n",
        "                                  )\n",
        "\n",
        "\n",
        "        self.Branch1_2 = nn.Sequential(\n",
        "                                  nn.BatchNorm2d(out_cha),\n",
        "                                  nn.ReLU(),\n",
        "\n",
        "                                  nn.Conv2d( out_cha,\n",
        "                                           out_cha,\n",
        "                                            kernel_size = 3,\n",
        "                                            stride= stride, padding = 1 ,\n",
        "                                            bias = False),\n",
        "\n",
        "                             )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.Branch1_1(input)\n",
        "        x =  self.Branch1_2(x) + self.shortcut(input)\n",
        "        return x\n",
        "class Wide_net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Wide_net, self).__init__()\n",
        "\n",
        "        self.start_covlayer = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, 1, 1, bias= False),\n",
        "\n",
        "           )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "\n",
        "                                    Branch(16, 16*k,  3, 1, 1,32),\n",
        "                                    HLA(16*k,  (3, 3), (2, 2), (1, 1), (32, 32), 16),\n",
        "\n",
        "                                    Branch(16*k, 16*k,  3, 1, 1, 32),\n",
        "                                    HLA(16*k,  (3, 3),(2, 2), (1, 1), (32, 32), 16),\n",
        "\n",
        "                                    # Branch(16*k, 16*k,  3, 1, 1, 32),\n",
        "                                    # HLA(16*k,  (3, 3), (2, 2), (1, 1), (32, 32), 16),\n",
        "\n",
        "                                    # Branch(16*k, 16*k,  3, 1, 1, 32),\n",
        "                                    # HLA(16*k,  (3, 3), (2, 2), (1, 1), (32, 32), 16),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                    )\n",
        "\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "\n",
        "                                    Branch(16*k, 32*k,  3, 2, 1,16),\n",
        "                                    HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "                                    Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    #HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    #HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    #HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    # HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "\n",
        "                                        Branch(32*k, 64*k,  3, 2, 1, 8),\n",
        "                                        squeeze(64*k,16),\n",
        "\n",
        "                                        Branch(64*k, 64*k, 3,1, 1, 8),\n",
        "\n",
        "                                        squeeze(64*k,16),\n",
        "\n",
        "                                        nn.BatchNorm2d(64*k),\n",
        "\n",
        "                                        nn.ReLU(),\n",
        "\n",
        "                                        nn.AdaptiveAvgPool2d(1) ,\n",
        "\n",
        "\n",
        "                                            )\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "\n",
        "        nn.Linear(64*k, num_classes),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.start_covlayer(x)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "\n",
        "        out = self.layer2(out)\n",
        "\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = Wide_net(num_classes).to(device)\n",
        "\n",
        "\n",
        "total_step = len(train_loader)\n",
        "num_epochs = 200\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum = 0.9, lr= 0.1,  weight_decay = 0.0005, nesterov=True)\n",
        "\n",
        "scheduler1 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120,160], gamma= 0.2)\n",
        "\n",
        "total_params = sum(param.numel() for param in model.parameters())\n",
        "\n",
        "with open('log_WRN_16_8_1.txt', 'w') as f:\n",
        "    f.write('Epochs\\tTraining Loss\\tTraining_acc\\tTesting_acc\\n')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    model.training = True\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total +=labels.size(0)\n",
        "        train_correct +=(predicted == labels).sum().item()\n",
        "\n",
        "    scheduler1.step()\n",
        "    train_acc = 100 * train_correct/train_total\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, train_acc: {:.3f} %'\n",
        "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item(), train_acc))\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    model.training = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
        "        with open('log_WRN_16_8_1.txt', 'a') as f:\n",
        "            f.write('[{}/{}]\\t{:.4f}\\t        {:.3f} %\\t      {:.2f} %\\n'.format(\n",
        "                epoch+1,num_epochs, loss.item(), train_acc, 100 * correct / total))\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}