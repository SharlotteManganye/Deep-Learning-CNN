{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOe5OiB4irge4Awy3kLakzm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharlotteManganye/Deep-Learning-CNN/blob/main/PuNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ProductUnitNN(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, output_dim):\n",
        "        super(ProductUnitNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.w1 = nn.Parameter(torch.randn(input_dim, hid_dim), requires_grad=True)\n",
        "        self.w2 = nn.Parameter(torch.randn(hid_dim, output_dim), requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.ones(output_dim) * -1, requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adjust net_sig computation\n",
        "        net_sig = torch.zeros((x.size(0), self.hid_dim), device=x.device)\n",
        "        for i in range(self.input_dim):\n",
        "            condition = x[:, i].unsqueeze(1) != 0\n",
        "            values_to_add = torch.sum(self.w1 * torch.log(torch.abs(x.unsqueeze(2) - 1)), dim=1)\n",
        "            # Expand condition to match net_sig's shape for broadcasting\n",
        "            condition = condition.expand_as(net_sig)  # Becomes (batch_size, hid_dim)\n",
        "            net_sig = torch.where(condition, values_to_add, net_sig)\n",
        "        z = net_sig @ self.w2 + self.bias\n",
        "        return z\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "input_dim = 784  # 28x28 images flattened\n",
        "hid_dim = 100    # Example hidden dimension\n",
        "output_dim = 10  # 10 classes for MNIST\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = ProductUnitNN(input_dim, hid_dim, output_dim)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Store gradient norms\n",
        "grad_norms = {name: [] for name, param in model.named_parameters()}\n",
        "\n",
        "# Training the model\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.view(-1, input_dim).to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # Calculate and store gradient norms\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                grad_norms[name].append(param.grad.norm().item())\n",
        "\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Testing the model\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.view(-1, input_dim).to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "# Plot gradient norms\n",
        "for name, norms in grad_norms.items():\n",
        "    plt.plot(norms, label=name)\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Gradient norm')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmeamQS9EVHu",
        "outputId": "968ef410-0714-4a89-ead4-c6f10b23195a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 15829507.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 475616.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3780528.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8787144.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 130.874069\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 175.440842\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 123.531250\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 80.761650\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 114.750122\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 108.515099\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 94.863319\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 64.042168\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 105.106049\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 68.801155\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 68.669281\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 29.518572\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 52.073051\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 39.826523\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 53.541702\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 23.297148\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 27.150322\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 19.769260\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 16.438759\n",
            "\n",
            "Test set: Average loss: 0.8784, Accuracy: 5173/10000 (52%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 27.756956\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 32.071480\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 18.438240\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 13.830185\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 14.800926\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 18.036175\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 15.456507\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 18.349731\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 20.864214\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 18.452528\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 25.465076\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 17.748169\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 13.433649\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 12.067994\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 28.403608\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 6.382118\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 15.609638\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 17.353462\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 8.491336\n",
            "\n",
            "Test set: Average loss: 0.3589, Accuracy: 6399/10000 (64%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 11.784608\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 12.358897\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 12.738282\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 8.194252\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 12.171802\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 7.686546\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 10.518480\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 11.203514\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 14.409605\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 12.316425\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 4.016611\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 3.424157\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 4.250412\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 5.327603\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 7.491172\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 10.770577\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 10.171404\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 10.623791\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 7.077757\n",
            "\n",
            "Test set: Average loss: 0.2097, Accuracy: 6720/10000 (67%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 7.481367\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 10.923351\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 5.229768\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 4.387077\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 7.158537\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 7.932600\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 4.148644\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 6.672404\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 4.824541\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 6.009888\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 7.826929\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 3.361587\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 3.012689\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 2.744311\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 3.752117\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 9.828362\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 7.982637\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 4.289996\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 4.199577\n",
            "\n",
            "Test set: Average loss: 0.1569, Accuracy: 6525/10000 (65%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.683117\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 5.851913\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 1.194104\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 3.073398\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 5.464055\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 9.004454\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.349453\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 3.356600\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.737679\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 3.639428\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 4.255422\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 5.250264\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 5.150252\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 2.814950\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 5.727729\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 3.402035\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 4.333448\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 4.218077\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.106223\n",
            "\n",
            "Test set: Average loss: 0.1283, Accuracy: 6643/10000 (66%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 3.194716\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 3.490798\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.729850\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 3.865294\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 4.959347\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 2.565836\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.386076\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 2.373285\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 3.935586\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 3.165875\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 6.130041\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 4.445030\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.384506\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 1.663790\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.396034\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 5.053610\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 5.697552\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 6.073280\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 5.515848\n",
            "\n",
            "Test set: Average loss: 0.1056, Accuracy: 6649/10000 (66%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.895349\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 1.866187\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.083292\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 3.409261\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 3.058101\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 2.489411\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 3.470970\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 2.122966\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 3.305164\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 1.231081\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 3.650065\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 3.955826\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.177151\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 2.416428\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.530091\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 3.215096\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.395733\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 2.907212\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.411268\n",
            "\n",
            "Test set: Average loss: 0.1016, Accuracy: 6515/10000 (65%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 4.297493\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 1.531074\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.402095\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 4.793510\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 4.523808\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 4.162243\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.581880\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 1.779744\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 3.917435\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 2.988188\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 5.593070\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 4.742276\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.127981\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 2.524568\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.763656\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 2.842627\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 3.934941\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 2.846343\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.114710\n",
            "\n",
            "Test set: Average loss: 0.0912, Accuracy: 6606/10000 (66%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.106706\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 3.089914\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.841842\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 3.374231\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.545482\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 4.612582\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.397745\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 3.402978\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.321491\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 2.083106\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.885067\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 2.803242\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.066878\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 1.714814\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 3.135359\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 4.825758\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 3.602940\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 3.129027\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.332915\n",
            "\n",
            "Test set: Average loss: 0.0818, Accuracy: 6609/10000 (66%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.972626\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 2.013699\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.258219\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 2.442130\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.984890\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.895025\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.506649\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 1.288923\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 4.186306\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 3.723495\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.445971\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 2.170403\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 4.489685\n"
          ]
        }
      ]
    }
  ]
}