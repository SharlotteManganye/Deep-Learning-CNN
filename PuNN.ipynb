{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHUUJ3jEZjoWq15M0ewAyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharlotteManganye/Deep-Learning-CNN/blob/main/PuNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "class ProductUnitNN(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, output_dim):\n",
        "        super(ProductUnitNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights\n",
        "        self.w1 = nn.Parameter(torch.randn(input_dim, hid_dim).to(device), requires_grad=True)\n",
        "        self.w2 = nn.Parameter(torch.randn(hid_dim, output_dim).to(device), requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.ones(output_dim) * -1, requires_grad=True).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # Adjust net_sig computation\n",
        "        net_sig = torch.zeros((x.size(0), self.hid_dim), device=x.device)\n",
        "        for i in range(self.input_dim):\n",
        "\n",
        "            condition = x[:, i].unsqueeze(1) != 0\n",
        "            values_to_add = torch.sum(self.w1 * torch.log(torch.abs(x.unsqueeze(2) - 1)), dim=1)\n",
        "  # Expand condition to match net_sig's shape for broadcasting\n",
        "            condition = condition.expand_as(net_sig)  # Becomes (batch_size, hid_dim)\n",
        "            net_sig = torch.where(condition, values_to_add, net_sig)\n",
        "\n",
        "\n",
        "        z = net_sig @ self.w2 + self.bias\n",
        "        return z\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "input_dim = 784  # 28x28 images flattened\n",
        "hid_dim = 100    # Example hidden dimension\n",
        "output_dim = 10  # 10 classes for MNIST\n",
        "\n",
        "model = ProductUnitNN(input_dim, hid_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.view(-1, input_dim).to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# Testing the model\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.view(-1, input_dim).to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmeamQS9EVHu",
        "outputId": "9a38d47a-9966-4da5-9466-a908b7cb00d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 250.750595\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 41.995422\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 28.185612\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 22.003904\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 24.125326\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 33.732098\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 44.457790\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 28.892536\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 43.986271\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 48.856533\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 31.915642\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 63.538059\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 50.298622\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 74.193909\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 44.727886\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 69.490128\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 81.768768\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 37.587097\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 46.495350\n",
            "\n",
            "Test set: Average loss: 1.9357, Accuracy: 5077/10000 (51%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 69.694908\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 77.493225\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 96.481552\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 42.337124\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 76.000687\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 90.827896\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 103.909172\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 53.086559\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 84.025421\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 63.526859\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 74.750099\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 61.720993\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 40.702492\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 23.271645\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 58.647190\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 63.906883\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 71.670883\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 87.383545\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 72.978928\n",
            "\n",
            "Test set: Average loss: 2.4287, Accuracy: 5055/10000 (51%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 120.211555\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 89.568207\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 70.213997\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 59.889530\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 49.728302\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 90.781586\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 62.603325\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 49.308788\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 91.714951\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 56.858028\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 112.834846\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 63.587917\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 80.255348\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 125.078468\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 51.143318\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 49.588860\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 32.915318\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 50.350945\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 92.606010\n",
            "\n",
            "Test set: Average loss: 2.9687, Accuracy: 5481/10000 (55%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 57.209274\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 81.550209\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 84.869797\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 45.883762\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 71.095291\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 103.900780\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 76.302597\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 93.044724\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 46.074146\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 64.231895\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 123.417259\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 61.744858\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 68.549225\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 69.675232\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 58.455334\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 73.284027\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 61.676022\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 41.207375\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 100.436279\n",
            "\n",
            "Test set: Average loss: 2.3886, Accuracy: 5633/10000 (56%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 62.433647\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 67.774689\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 84.015266\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 104.794243\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 77.379005\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 79.991859\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 129.396027\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 85.756271\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 98.695206\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 46.597279\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 97.324661\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 85.414650\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 110.228989\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 70.238571\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 106.822205\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 122.039001\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 136.766281\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 99.250839\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 76.567734\n",
            "\n",
            "Test set: Average loss: 2.3374, Accuracy: 5658/10000 (57%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 66.088768\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 68.903732\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 65.615173\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 80.397583\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 113.012756\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 78.314575\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 79.692497\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 66.017456\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 85.550827\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 91.386833\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 85.066620\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 100.928871\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 68.830078\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 49.211422\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 108.032669\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 89.496895\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 40.310223\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 58.315365\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 41.801937\n",
            "\n",
            "Test set: Average loss: 2.6241, Accuracy: 5930/10000 (59%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 36.798409\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 68.524467\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 96.359871\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 80.658890\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 116.833054\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 104.941277\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 106.456375\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 53.565720\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 85.595390\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 46.246216\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 125.391022\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 77.400177\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 76.781570\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 78.241379\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 84.473091\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 34.850620\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 90.833977\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 61.933941\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 82.790123\n",
            "\n",
            "Test set: Average loss: 2.8982, Accuracy: 5801/10000 (58%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 77.661682\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 186.420303\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 71.775253\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 85.339874\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 122.507652\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 81.053383\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 74.847702\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 117.288940\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 85.248688\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 68.110519\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 58.440742\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 49.134533\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 78.741936\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 109.193069\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 50.031136\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 83.756508\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 117.362152\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 140.497498\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 75.920303\n",
            "\n",
            "Test set: Average loss: 2.5475, Accuracy: 5866/10000 (59%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 74.980774\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 145.539978\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 106.114357\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 40.845165\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 103.543236\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 129.468994\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 67.532166\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 112.455719\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 146.594742\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 112.828606\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 122.496681\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 62.257782\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 87.545036\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 39.626205\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 74.717186\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 58.554058\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 115.051369\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 52.108322\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 68.861107\n",
            "\n",
            "Test set: Average loss: 2.7142, Accuracy: 5215/10000 (52%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 102.025497\n",
            "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 71.478111\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 70.557503\n",
            "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 138.360748\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 89.880486\n",
            "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 81.514282\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 95.154427\n",
            "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 48.591682\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 78.280678\n",
            "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 69.655136\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 96.284119\n",
            "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 89.565201\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 53.735977\n",
            "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 52.734955\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 62.261276\n",
            "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 55.948528\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 98.045372\n",
            "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 94.855606\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 141.854507\n",
            "\n",
            "Test set: Average loss: 2.5708, Accuracy: 6007/10000 (60%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}