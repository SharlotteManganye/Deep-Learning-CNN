{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtd83zSZZd/Gm6rKAsCK/h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharlotteManganye/Deep-Learning-CNN/blob/main/Mnist_wider_HLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "18LDq8nMpW0W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.autoaugment import AutoAugmentPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficient_High_order_conv file"
      ],
      "metadata": {
        "id": "ILM6Y4X0pY3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def PM_creation( N, input_size, TRM):\n",
        "    index = []\n",
        "    PM = torch.arange(input_size)[:,None]\n",
        "    TPM = []\n",
        "    TPM.append(PM)\n",
        "    PM_full = []\n",
        "    for i in range(N-1):\n",
        "        Nc = int(PM.shape[1])\n",
        "        for j in range(input_size-Nc):\n",
        "            j = j+1\n",
        "            p_N1 = (PM[:,Nc-1] + j)[:, None]\n",
        "            index_chosen = torch.argwhere(p_N1 < input_size)\n",
        "            index_final = torch.concat((PM[index_chosen[:,0]],p_N1[index_chosen[:,0]]), axis = 1)\n",
        "            index.append(index_final)\n",
        "            p_N1 = p_N1 - j\n",
        "        PM = torch.vstack(index)\n",
        "        index = []\n",
        "        TPM.append(PM)\n",
        "    for k in range(len(TRM)):\n",
        "        PM = TPM[len(TRM[k])-1]\n",
        "        PM_full.append(np.repeat(PM, TRM[k], axis =1))\n",
        "    PM_full = torch.vstack(PM_full)\n",
        "    return PM_full\n",
        "\n",
        "\n",
        "def PCM_creation(PM_full_r_1, PM_full_r):\n",
        "    PCM_r = []\n",
        "    PCMs_r = []\n",
        "    for i in range(PM_full_r.shape[1]):\n",
        "        PM_deleted = torch.cat((PM_full_r[:, :i], PM_full_r[:, i + 1:]), dim=1)\n",
        "        for row_r in PM_deleted:\n",
        "            Position = torch.where(torch.all(PM_full_r_1 ==row_r, axis = 1))\n",
        "            PCM_r.append(torch.tensor(Position))\n",
        "        PCM_r = torch.vstack(PCM_r)\n",
        "        PCM_r = torch.concat((PM_full_r[:,i][:,None], PCM_r), axis = 1)\n",
        "        PCMs_r.append(PCM_r)\n",
        "        PCM_r = []\n",
        "    return  PCMs_r\n",
        "\n",
        "\n",
        "class HT_creation(Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input, kernel_size, stride, padding,\n",
        "                  TPCMs_r):\n",
        "            input = input.to(device)\n",
        "            shape = input.shape\n",
        "            grad_move = []\n",
        "            grad_move.append(0)\n",
        "            x_r_1_move =[]\n",
        "            x_r_1_move.append(0)\n",
        "            HT = []\n",
        "            unfold = nn.Unfold(kernel_size = kernel_size,  padding = padding, stride = stride)\n",
        "            Col = unfold(input)\n",
        "            weight_shape = []\n",
        "\n",
        "            ori_col = int(kernel_size[0]*kernel_size[1])\n",
        "            Col = Col.view(shape[0], shape[1], ori_col, Col.shape[-1])\n",
        "\n",
        "            for i in range(len(TPCMs_r)):\n",
        "                if i == 0:\n",
        "                    grad_move.append(grad_move[i]+TPCMs_r[i][0].shape[0])\n",
        "                    weight_shape.append((ori_col, ori_col))\n",
        "                    HT.append(Col[:,:,TPCMs_r[i][0][:,0],:]*Col[:,:,TPCMs_r[i][0][:,1],:])\n",
        "                    x_r_1_move.append(x_r_1_move[i]+Col.shape[2])\n",
        "                else:\n",
        "                    grad_move.append(grad_move[i]+TPCMs_r[i][0].shape[0])\n",
        "                    weight_shape.append((TPCMs_r[i-1][0].shape[0], ori_col))\n",
        "                    HT.append(Col[:,:,TPCMs_r[i][0][:,0],:]*HT[i-1][:,:,TPCMs_r[i][0][:,1],:])\n",
        "                    x_r_1_move.append(x_r_1_move[i]+TPCMs_r[i-1][0].shape[0])\n",
        "            HT = torch.cat(HT, dim = 2)\n",
        "            if len(TPCMs_r) == 1:\n",
        "                # ctx.back_terms = Col\n",
        "                ctx.save_for_backward(Col)\n",
        "            else:\n",
        "                # ctx.back_terms = torch.cat((Col,HT[:,:,0:grad_move[-2],:]), dim=2 )\n",
        "                ctx.save_for_backward(torch.cat((Col,HT[:,:,0:grad_move[-2],:]), dim=2 ))\n",
        "            ctx.grad_move = grad_move\n",
        "            ctx.x_r_1_move = x_r_1_move\n",
        "            ctx.TPCMs_r = TPCMs_r\n",
        "\n",
        "            ctx.weight_shape = weight_shape\n",
        "            ctx.kernel_size =kernel_size\n",
        "            ctx.stride = stride\n",
        "            ctx.padding = padding\n",
        "            ctx.shape =shape\n",
        "\n",
        "            return HT\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad):\n",
        "            (back_terms,) = ctx.saved_tensors\n",
        "\n",
        "            # back_terms = ctx.back_terms\n",
        "            weight_shape = ctx.weight_shape\n",
        "            TPCMs_r = ctx.TPCMs_r\n",
        "            kernel_size = ctx.kernel_size\n",
        "            stride = ctx.stride\n",
        "            padding = ctx.padding\n",
        "            shape = ctx.shape\n",
        "            grad_move = ctx.grad_move\n",
        "            x_r_1_move = ctx.x_r_1_move\n",
        "            grad_input  = None\n",
        "\n",
        "            for i in range(len(TPCMs_r)):\n",
        "                Al_grad = torch.zeros((grad.shape[0], grad.shape[1],\n",
        "                                          weight_shape[i][0], weight_shape[i][1],grad.shape[-1])).to(device)\n",
        "                if i == 0:\n",
        "                    Al_grad[:,:, TPCMs_r[0][0][:,0],TPCMs_r[0][0][:,1],:] = grad[:,:,grad_move[0]:grad_move[1],:]\n",
        "                    # Al_grad[:,:, PCMs_r[0][:,1],PCMs_r[0][:,0],:] += grad[:,:,grad_move[0]:grad_move[1],:]\n",
        "                    Al_grad = Al_grad + Al_grad.transpose(2,3)\n",
        "                    grad_input = torch.einsum('bcikj,bckj->bcij',Al_grad,\n",
        "                                              back_terms[:,:,x_r_1_move[0]:x_r_1_move[1],:])\n",
        "                else:\n",
        "                    Al_grad[:,:, TPCMs_r[i][0][:,1], TPCMs_r[i][0][:,0],:] = grad[:,:,grad_move[i]:grad_move[i+1],:]\n",
        "                    for j in range(i+1):\n",
        "                        Al_grad[:,:, TPCMs_r[i][j+1][:,1], TPCMs_r[i][j+1][:,0],:] +=grad[:,:,grad_move[i]:grad_move[i+1],:]\n",
        "                    grad_input += torch.einsum('bckij,bckj->bcij',Al_grad,\n",
        "                                                back_terms[:,:,x_r_1_move[i]:x_r_1_move[i+1],:])\n",
        "            grad_input = grad_input.reshape(grad_input.shape[0],\n",
        "                                          grad_input.shape[1]*grad_input.shape[2], grad_input.shape[-1])\n",
        "            fold = nn.Fold((shape[-2], shape[-1]), kernel_size =kernel_size, stride = stride, padding = padding )\n",
        "            grad_input = fold(grad_input)\n",
        "\n",
        "            return grad_input, None, None, None, None, None\n",
        "\n",
        "class high_order_input(nn.Module):\n",
        "    def __init__(self, kernel_size, stride, padding, TPCMs_r):\n",
        "        super().__init__()\n",
        "        self.func = HT_creation\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.TPCMs_r = TPCMs_r\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.func.apply(input,\n",
        "                          self.kernel_size,\n",
        "                          self.stride,\n",
        "                          self.padding,\n",
        "                          self.TPCMs_r\n",
        "                          )\n",
        "\n",
        "\n",
        "class HO_conv(nn.Module):\n",
        "    def __init__(self, in_chan , out_chan, kernel_size, stride, padding, groups, TPCMs_r):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding =  padding\n",
        "        self.stride = stride\n",
        "        self.TPCMs_r = TPCMs_r\n",
        "        self.H_kernel_size = 0\n",
        "        for i in range(len(self.TPCMs_r)):\n",
        "\n",
        "            size = self.TPCMs_r[i][0].shape[0]\n",
        "\n",
        "            self.H_kernel_size += size\n",
        "        self.conv =  nn.Sequential( high_order_input( self.kernel_size,\n",
        "                                                self.stride,\n",
        "                                                self.padding,\n",
        "                                                self.TPCMs_r),\n",
        "                                    nn.Conv2d(in_chan, out_chan,\n",
        "                                              kernel_size=(self.H_kernel_size,1),\n",
        "                                              stride = 1,\n",
        "                                              groups = groups, bias = False))\n",
        "    def forward(self,input):\n",
        "        in_shape = input.shape\n",
        "        x = self.conv(input)\n",
        "        x =  x.reshape(x.shape[0],\n",
        "                        x.shape[1],\n",
        "                      (in_shape[-2]-self.kernel_size[0]+ 2* self.padding[0])//self.stride[0] + 1,\n",
        "                      (in_shape[-1]-self.kernel_size[1]+ 2* self.padding[1])//self.stride[1] + 1 )\n",
        "        return x"
      ],
      "metadata": {
        "id": "duVvvwe8pfAb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wider_net_HLA file"
      ],
      "metadata": {
        "id": "gvt70QioqDIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "      transforms.RandomCrop(28, padding = 4),\n",
        "     transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
        "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with MNIST mean and std\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset with data augmentation\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "k = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class squeeze(nn.Module):\n",
        "    def __init__(self, in_chan, reduce):\n",
        "        super(squeeze, self).__init__()\n",
        "\n",
        "\n",
        "        self.squ = nn.Sequential(\n",
        "                                    nn.AdaptiveAvgPool2d(1),\n",
        "                                    nn.Conv2d(in_chan, in_chan//reduce,1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(in_chan//reduce,in_chan, 1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "    def forward(self, input):\n",
        "        x = self.squ(input)*input\n",
        "        return x\n",
        "\n",
        "class Volconv(nn.Module):\n",
        "    def __init__(self, in_chan,out_chan, kernel_size, stride, padding, groups):\n",
        "        super(Volconv, self).__init__()\n",
        "        input_size = int(kernel_size[0]*kernel_size[1])\n",
        "        # PM1_Full = torch.arange(input_size)[:,None]\n",
        "        TRM2 = [[2],[1,1]]\n",
        "        PM2_Full = PM_creation(2, input_size, TRM2)\n",
        "        # TRM3 = [[3],[1,2],[2, 1], [ 1, 1, 1]]  # Third order Toe\n",
        "        # PM3_Full = PM_creation(3, input_size, TPE3)\n",
        "        TPCMs_r = [[PM2_Full]]\n",
        "        # PCMs_r = PCM_creation(PM2_Full, PM3_Full)\n",
        "\n",
        "        self.Second_conv = nn.Sequential(\n",
        "\n",
        "                                   HO_conv(in_chan,  out_chan,\n",
        "                                              kernel_size  ,\n",
        "                                              stride  , padding, groups, TPCMs_r\n",
        "                                              ), )\n",
        "\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "                                   nn.Conv2d(in_chan, out_chan, kernel_size,\n",
        "                                              stride , padding, groups = groups, bias = False))\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.Second_conv(input)+self.conv(input)\n",
        "\n",
        "        return x\n",
        "class HLA(nn.Module):\n",
        "    def __init__(self, in_chan, kernel_size, pooling_size, padding,  in_size, reduce):\n",
        "        super(HLA, self).__init__()\n",
        "        up_size1 = in_size[0]//pooling_size[0]\n",
        "        up_size2 = in_size[1]//pooling_size[1]\n",
        "\n",
        "        self.Vol1= nn.Sequential(   nn.BatchNorm2d( in_chan),\n",
        "                                    nn.ReLU(),\n",
        "\n",
        "                                    nn.AdaptiveAvgPool2d((pooling_size[0], pooling_size[1])),\n",
        "\n",
        "                                    nn.Conv2d(in_chan, in_chan//reduce, kernel_size=1,\n",
        "                                                stride=1 , padding = 0, groups = 1),\n",
        "                                    nn.BatchNorm2d( in_chan//reduce),\n",
        "\n",
        "                                    Volconv(in_chan//reduce,in_chan//reduce, kernel_size, (1, 1), padding, 1),\n",
        "\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d( in_chan//reduce),\n",
        "\n",
        "                                    nn.Conv2d(in_chan//reduce, in_chan, kernel_size=1,\n",
        "                                                stride=1 , padding = 0, groups = 1),\n",
        "\n",
        "                                    nn.Sigmoid(),\n",
        "\n",
        "                                    )\n",
        "        self.sq = nn.Sequential(\n",
        "                                    nn.AdaptiveAvgPool2d(1),\n",
        "                                    nn.Conv2d(in_chan, in_chan//reduce, 1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Conv2d(in_chan//reduce, in_chan, 1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor= (up_size1,up_size2) )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x1 = self.Vol1(input)\n",
        "        x2 = self.sq(input)\n",
        "        x3 = torch.mean(x2, dim=1, keepdim= True)\n",
        "        x2 = x3 - nn.ReLU()(x3 - x2)\n",
        "        x = x2 + x1 - x2*x1\n",
        "        x = self.up(x)*input\n",
        "        return x\n",
        "\n",
        "class Branch(nn.Module):\n",
        "    def __init__(self,\n",
        "                  in_cha,\n",
        "                  out_cha,\n",
        "                  kernel_size,\n",
        "                  stride,\n",
        "                  padding,\n",
        "                  in_size\n",
        "                  ):\n",
        "\n",
        "        super(Branch, self).__init__()\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_cha != out_cha or stride != 1:\n",
        "            self.shortcut = nn.Sequential( nn.Conv2d(in_cha,\n",
        "                        out_cha,\n",
        "                        kernel_size=1,\n",
        "                        stride= stride  , padding=0,\n",
        "                      bias = False),)\n",
        "\n",
        "        self.Branch1_1 = nn.Sequential(\n",
        "                                  nn.BatchNorm2d( in_cha),\n",
        "                                  nn.ReLU(),\n",
        "\n",
        "                                  nn.Conv2d(in_cha,\n",
        "                                            out_cha,\n",
        "                                            kernel_size = 3,\n",
        "                                            stride= 1, padding = 1,\n",
        "                                            bias = False),\n",
        "\n",
        "                                  )\n",
        "\n",
        "\n",
        "        self.Branch1_2 = nn.Sequential(\n",
        "                                  nn.BatchNorm2d(out_cha),\n",
        "                                  nn.ReLU(),\n",
        "\n",
        "                                  nn.Conv2d( out_cha,\n",
        "                                           out_cha,\n",
        "                                            kernel_size = 3,\n",
        "                                            stride= stride, padding = 1 ,\n",
        "                                            bias = False),\n",
        "\n",
        "                             )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.Branch1_1(input)\n",
        "        x =  self.Branch1_2(x) + self.shortcut(input)\n",
        "        return x\n",
        "class Wide_net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Wide_net, self).__init__()\n",
        "\n",
        "        self.start_covlayer = nn.Sequential(\n",
        "            nn.Conv2d(1, 14, 3, 1, 1, bias= False),\n",
        "\n",
        "           )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "\n",
        "                                    Branch(14, 14*k,  3, 1, 1,28),\n",
        "                                    HLA(14*k,  (3, 3), (2, 2), (1, 1), (28, 28), 14),\n",
        "\n",
        "                                    Branch(14*k, 14*k,  3, 1, 1, 28),\n",
        "                                    HLA(14*k,  (3, 3),(2, 2), (1, 1), (28, 28), 14),\n",
        "\n",
        "                                    # Branch(16*k, 16*k,  3, 1, 1, 32),\n",
        "                                    # HLA(16*k,  (3, 3), (2, 2), (1, 1), (32, 32), 16),\n",
        "\n",
        "                                    # Branch(16*k, 16*k,  3, 1, 1, 32),\n",
        "                                    # HLA(16*k,  (3, 3), (2, 2), (1, 1), (32, 32), 16),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                                    )\n",
        "\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "\n",
        "                                    Branch(14*k, 28*k,  3, 2, 1,14),\n",
        "                                    HLA(28*k,  (3, 3), (2, 2), (1, 1), (14, 14), 14),\n",
        "\n",
        "                                    Branch(28*k, 28*k,3,1, 1, 14),\n",
        "                                    HLA(28*k,  (3, 3), (2, 2), (1, 1), (14, 14), 14),\n",
        "\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    #HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    #HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    #HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "\n",
        "                                    # Branch(32*k, 32*k,3,1, 1, 16),\n",
        "                                    # HLA(32*k,  (3, 3), (2, 2), (1, 1), (16, 16), 16),\n",
        "\n",
        "                                    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "\n",
        "                                        Branch(28*k, 64*k,  3, 2, 1, 8),\n",
        "                                        squeeze(64*k,14),\n",
        "\n",
        "                                        Branch(64*k, 64*k, 3,1, 1, 8),\n",
        "\n",
        "                                        squeeze(64*k,14),\n",
        "\n",
        "                                        nn.BatchNorm2d(64*k),\n",
        "\n",
        "                                        nn.ReLU(),\n",
        "\n",
        "                                        nn.AdaptiveAvgPool2d(1) ,\n",
        "\n",
        "\n",
        "                                            )\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "\n",
        "        nn.Linear(64*k, num_classes),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.start_covlayer(x)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "\n",
        "        out = self.layer2(out)\n",
        "\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = Wide_net(num_classes).to(device)\n",
        "\n",
        "\n",
        "total_step = len(train_loader)\n",
        "num_epochs = 200\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), momentum = 0.9, lr= 0.1,  weight_decay = 0.0005, nesterov=True)\n",
        "\n",
        "scheduler1 = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120,160], gamma= 0.2)\n",
        "\n",
        "total_params = sum(param.numel() for param in model.parameters())\n",
        "\n",
        "with open('log_WRN_16_8_1.txt', 'w') as f:\n",
        "    f.write('Epochs\\tTraining Loss\\tTraining_acc\\tTesting_acc\\n')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    model.training = True\n",
        "    train_total = 0\n",
        "    train_correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total +=labels.size(0)\n",
        "        train_correct +=(predicted == labels).sum().item()\n",
        "\n",
        "    scheduler1.step()\n",
        "    train_acc = 100 * train_correct/train_total\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, train_acc: {:.3f} %'\n",
        "                    .format(epoch+1, num_epochs, i+1, total_step, loss.item(), train_acc))\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    model.training = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "\n",
        "        print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
        "        with open('log_WRN_16_8_1.txt', 'a') as f:\n",
        "            f.write('[{}/{}]\\t{:.4f}\\t        {:.3f} %\\t      {:.2f} %\\n'.format(\n",
        "                epoch+1,num_epochs, loss.item(), train_acc, 100 * correct / total))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kf31yRKOqTE2",
        "outputId": "7c976f91-d72f-4dd1-94b0-cdd79000221e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c6a8a69e77be>\u001b[0m in \u001b[0;36m<cell line: 328>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c6a8a69e77be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_covlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c6a8a69e77be>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBranch1_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBranch1_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OWRvZTCvp1Eb"
      }
    }
  ]
}